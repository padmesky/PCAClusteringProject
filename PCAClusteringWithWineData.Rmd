---
title: "Predicting Wine Quality with PCA Clustering Algorithm"
subtitle: "PCA Clustering Algorithm"
date: "`r format(Sys.time(), '%d %B %Y')`"
author: "Havva Nur Elveren"
output:
  html_document:
      theme: journal
      toc: yes
      toc_depth: 4
      #toc_float: true
  word_document:
      toc: yes
      toc_depth: 4
      #toc_float: true
  pdf_document:
      toc: yes
      theme: journal
      toc_depth: 4
      #toc_float: true
---
---
# Objective: Predicting Wine Quality
Can we predict wine quality based on its features such as acidity, alcohol, sugar or sulfate level? In this project, we'll predict Wine Quality with looking at the value of different features of a wine. We'll use a data set that has been collected from red wine variants of the Portuguese "Vinho Verde" wine. If quality is greater than 6.5 it is considered as good wine, otherwise it is considered as bad wine.

# Data Description:
* 1.6K Row with 12 Column. You can download the data from the link https://www.kaggle.com/uciml/red-wine-quality-cortez-et-al-2009
```{r}
library(kableExtra)

dt <- data.frame(Name = c("fixed.acidity", "volatile.acidity", "citric.acid", "residual.sugar", "chlorides", "free.sulfur.dioxide", "total.sulfur.dioxide",
"density", "pH", "sulphates", "alcohol", "quality"),
Description = c("most acids involved with wine or fixed or nonvolatile (do not evaporate readily)", "the amount of acetic acid in wine, which at too high of levels can lead to an unpleasant, vinegar taste", "found in small quantities, citric acid can add 'freshness' and flavor to wines", "the amount of sugar remaining after fermentation stops, it's rare to find wines with less than 1 gram/liter and wines with greater than", "the amount of salt in the wine", "the free form of SO2 exists in equilibrium between molecular SO2 (as a dissolved gas) and bisulfite ion; it prevents", "amount of free and bound forms of S02; in low concentrations, SO2 is mostly undetectable in wine, but at free SO2", "the density of water is close to that of water depending on the percent alcohol and sugar content", "describes how acidic or basic a wine is on a scale from 0 (very acidic) to 14 (very basic); most wines are between 3-4 on the", "a wine additive which can contribute to sulfur dioxide gas (S02) levels, wich acts as an antimicrobial and", "the percent alcohol content of the wine","Score between 0 and 10, if quality > 6.5 it's Good, otherwise it is Bad "))

dt %>%
  kbl() %>%
  kable_styling()

```
# Loading the Libraries
```{r message=FALSE, warning=FALSE}
set.seed(7)
library(gmodels)
library(caret)
library(mlbench)
library(caret)
library(clustertend)
library(factoextra)
library(readxl)
library(ggplot2)
library(dplyr)
library(corrplot)
library(caret)
library(cluster)
library(factoextra)
library(magrittr)
library(fpc)
library(ggfortify)
library(psych)
library(readr)
```
# Loading the Data Set
```{r message=FALSE, warning=FALSE}
winedata <- read.csv("winequality-red.csv")
winedata <- data.frame(winedata, stringsAsFactors = FALSE, colnames<- TRUE )

str(winedata)
summary(winedata)
head(winedata)
table(winedata$quality)

```
## Visualizing the data and Exploring the data using PCA 

```{r message=FALSE, warning=FALSE}

#Visualize the correlation between features.
pairs.panels(winedata[c("fixed.acidity","volatile.acidity","citric.acid","residual.sugar","chlorides","sulphates", "alcohol","free.sulfur.dioxide","total.sulfur.dioxide","density","pH","quality")])

transformed_pca = preProcess(winedata[,c("fixed.acidity","volatile.acidity","citric.acid","residual.sugar","chlorides","free.sulfur.dioxide","total.sulfur.dioxide","density","pH", "sulphates","alcohol","quality" )], method=c("BoxCox", "center", "scale", "pca"))
PC = predict(transformed_pca, winedata[,c("fixed.acidity","volatile.acidity","citric.acid","residual.sugar","chlorides","free.sulfur.dioxide","total.sulfur.dioxide","density","pH", "sulphates","alcohol","quality" )], method=c("BoxCox", "center", "scale", "pca"))
head(PC, 6)

```

## Using k-means Clustering Algorithm
```{r message=FALSE, warning=FALSE}
# Plot the principal components in our data without scaling the data 
pc <- prcomp(winedata)
summary(pc)
plot(pc)

# As shown in the principal component plot, we only have two groups.
# Verify by plotting variance of columns to see if we need to center and scale the data
mar <- par()$mar
par(mar=mar+c(0,5,0,0))
barplot(sapply(winedata, var), horiz=T, las=1, cex.names=0.8)
par(mar=mar)
# In the plot, we see that value of total.sulfur.dioxide and free.sulfur.dioxide columns have greater variance comparing to other columns. To avoid those values to overrule the other predictor variables, we'll scale our data.

# Scale the data
winedata$total.sulfur.dioxide <- scale(winedata$total.sulfur.dioxide)
winedata$free.sulfur.dioxide <- scale(winedata$free.sulfur.dioxide)

pc <- prcomp(winedata)
plot(pc)
# As we can see after scaling the data, our data has been spread into more groups.

# Use Elbow method to see the optimal k value for the kmeans
elbow_figure<-fviz_nbclust(winedata, kmeans, method = "wss")
elbow_figure

# Use Silhouette score to see the optimal k value for the kmeans
sil_score<-fviz_nbclust(winedata, kmeans, method = "silhouette")
sil_score

# Use Gap Statistics to see the optimal k value for the kmeans
set.seed(123)
gap_stat <- clusGap(winedata, FUN = kmeans, nstart = 25,
                    K.max = 10, B = 50)
fviz_gap_stat(gap_stat)

# Plot the K-means clustering with k=2
kmeans_clust<-kmeans(winedata,2)
str(kmeans_clust)
autoplot(kmeans_clust, data=winedata)

# Try K-means clustering with k=5
kmeans_clust<-kmeans(winedata,5)
str(kmeans_clust)
autoplot(kmeans_clust, data=winedata)

```

## Applying PCA Clustering with Linear Model

```{r message=FALSE, warning=FALSE}

train_idx <- createDataPartition(winedata$quality, p = 0.75, list = FALSE)
trainData <- winedata[train_idx,]
testData <- winedata[-train_idx,]

# Center, scaling, transforming with BoxCox and applying PCA. Leave out the target variable quality. Use only 2 PCA
preProc  <- preProcess(trainData[,-12], method = c("BoxCox","center", "scale", "pca"), pcaComp = 2) # or 
train_pca <- predict(preProc, trainData[,-12])

# Train and test for the transformed data and adding the column vector that needs to be predicted
train_pca$quality <- trainData$quality
test_pca <- predict(preProc, testData[,-12])
head(test_pca)
test_pca$quality <- testData$quality
head(train_pca)

# Fitting a linear model using components.
fit <- lm(quality~., data = train_pca)
print(fit$coefficients)
summary(fit)

# Predicting using the pca components
predictions<-predict(fit,test_pca)
head(predictions)

# Root mean Squared error
rmse<-RMSE( predictions, testData$quality)
rmse

```

# Conclusion
In this project we applied pca clustering and kmeans clustering algorithms on our wine data to predict wine quality. 
Before applying the clustering algorithms on our data, first we examined our data to see the correlation between our variables. But our correlation graph shows that there is no strong correlation between our variables. Also our data is not balanced, we don't have same amount of data for each wine quality score, but since our data is not large, we don't process the data to balance it.

First we look at the principal components without processing the data, and we see a right skewed graph with 2 component. When we plot the variable variance of each column, we see that some of the variables have larger variance comparing to others, which results those variables to overrule the others. To avoid this issue, we scale our data and get the principle components again and this time we see a better result. After scaling our data, data is distributed into different clusters in a better way.

To get the best k value to use kmeans clustering algorithm, we use Elbow method, Silhouette score and Gap statistic. According to elbow method and silhouette score 2 seems to be the optimal k value. Gap statistic shows 1 for the optimal k value. We pick 2 as k value and apply kmeans clustering algorithm to our data. And then we use 5 as k value to see if it makes any improvement on our clustering. However increasing the number of clusters doesn't provide any improvement and causes overlaps.

Once we decide on the optimal k value, we apply pca clustering with 2 pca with linear regression model. And make the prediction with this model. If we look at the result, Adjusted R-squared value as 0.343 is pretty low and it means this is not a strong model. 

Since there is not a strong correlation between our variables and our data is unbalanced, clustering algorithm doesn't seems to be a good choice to predict wine quality with our data set. 


